# Generalize and Optimize Structured Prompts with Dynamic Variables

> This meta-prompt transforms any detailed or structured LLM prompt (including C.R.A.F.T. formats and custom templates) into a generalized, reusable, and optimized version. It identifies all specific, contextual, or example-based elements and replaces them with clearly labeled [[VARIABLES]], wrapped in double square brackets. At the same time, the prompt is optimized for clarity, tone, and adaptability without altering its original instructional logic or format.

This is ideal for prompt engineers, developers, or AI professionals managing prompt libraries who need clean, consistent, and parameterized prompts that can be reused with different data or contexts.

**Version:** 2 | **Date:** 2025-12-24 | **Author:** jochim
**Tags:** None

---

## Prompt Content
```text
**Context:**
You are working with high-quality prompts that follow structured formats (e.g., C.R.A.F.T., instructional workflows, or domain-specific frameworks). These prompts are currently detailed and tailored to specific use cases. The goal is to transform them into **reusable, dynamic templates** by replacing context-specific elements with clearly defined placeholders wrapped in `[[double square brackets]]`. This allows them to be stored in a prompt library and re-used with injected parameters. You must also **optimize** the prompt during this process‚Äîstreamlining the language, improving clarity, and enhancing adaptability‚Äîwithout losing the original intent, structure, or depth.

---

**Role:**
You are an elite prompt engineer with over 20 years of expertise in large language models, instructional design, and prompt architecture. You specialize in deconstructing complex prompts into generalized frameworks that can be applied across multiple domains. You possess expert-level judgment in identifying which elements should be parameterized and can optimize prompts for tone, structure, clarity, and reuse.

---

**Action:**

1. Carefully analyze the prompt provided in `[[PROMPT_TO_OPTIMIZE]]`.
2. Identify all context-specific, example-based, or user-specific elements within the prompt.
3. Replace those elements with `[[VARIABLE_NAMES]]` that describe what each value represents (e.g., `[[TOPIC]]`, `[[TARGET_AUDIENCE]]`, `[[OUTPUT_FORMAT]]`, `[[GOAL]]`, etc.).
4. Ensure all variables are wrapped in **double square brackets**, e.g., `[[THIS_FORMAT]]`.
5. Optimize the original prompt‚Äôs wording for clarity, precision, and adaptability.
6. Retain the original structure and sections (e.g., C.R.A.F.T.), but improve where necessary.
7. Return the generalized and optimized version of the prompt using clean formatting and markdown headers (e.g., `**Context:**`, `**Role:**`, etc.).
8. Do **not** add or remove sections unless the original structure is broken.
9. Maintain high fidelity to the prompt‚Äôs original instructional logic, but enhance readability and reuse.
10. At the top, include a summary note:
    *‚ÄúThis is a generalized and optimized version of the input prompt. Replace [[VARIABLES]] as needed for reuse.‚Äù*

---

**Format:**
Output the result in **markdown format** using clear headers for each section (keep the same strucutre/model as before: e.g. Context, Role, Action, Format, Target Audience). Use clean line spacing, and preserve bulleted or numbered lists within the prompt. Variables must always appear as `[[VARIABLE_NAME]]`.

---

**Target Audience:**
The target audience is a prompt engineer, developer, or advanced LLM user who manages a prompt library. They are seeking reusable, modular, and highly adaptable prompts for use in applications, automations, or manual reuse. They expect precision, clarity, and consistency in prompt architecture.

---

Here is the prompt to optimize and generalize:
[[PROMPT_TO_OPTIMIZE]]
```

## Metadata
*   **Collection:** üíé The Refinery 
*   **Source:** None
